{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 0031 - Introduction to Natural Language Processing\n",
    "In this lesson, we will make our first steps with natural language processing.<br>\n",
    "We start by importing [nltk](http://www.nltk.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "print( nltk.__version__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with __tokenization__: to tokenize means to break down a sentence into a list of the words that compose that sentence. For this end, we employ [word_tokenize](http://www.nltk.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'my', 'first', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "mytext = \"This is my first sentence\"\n",
    "\n",
    "tokens = nltk.word_tokenize( mytext )\n",
    "\n",
    "print( tokens )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we consider __stemming__: stemming a word means to cut syllables so that only the stem of the given word remains. For this, we employ the [PorterStemmer](http://www.nltk.org/howto/stem.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "care\n",
      "hopeless\n",
      "die\n",
      "peac\n",
      "be\n",
      "bat\n",
      "feet\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer( )\n",
    "\n",
    "list_of_words = [ 'caring', 'hopelessness', 'dies', 'peaceful', 'being', 'bats', 'feet' ]\n",
    "\n",
    "for word in list_of_words:\n",
    "    \n",
    "    print( stemmer.stem( word ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the list above, we find \"words\" that do not really exist like \"peac\".<br>\n",
    "__Lemmatization__ is similar to __stemming__ with the difference, that __lemmatization__ only returns actual words. For this, we employ the [WordNetLemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caring\n",
      "hopelessness\n",
      "dy\n",
      "peaceful\n",
      "being\n",
      "bat\n",
      "foot\n"
     ]
    }
   ],
   "source": [
    "lemma = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "for word in list_of_words:\n",
    "    \n",
    "    print( lemma.lemmatize( word ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compare the two resulting lists, we notice, that the __PorterStemmer__ returned the true stems for __caring__, __hopelessness__ and __dies__, but failed at __peaceful__. We also notice, that the __WordNetLemmatizer__ successfully wranlged __feet__ to __foot__. Let us reconsider __caring__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "care\n"
     ]
    }
   ],
   "source": [
    "print( lemma.lemmatize( 'caring', 'v' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caring\n"
     ]
    }
   ],
   "source": [
    "print( lemma.lemmatize( 'caring', 'n' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two examples underline the importance of finding out, what kind of word a certain word actually is, because the __WordNetLemmatizer__ can correctly reduce __caring__ to __care__ once we tell it, that __caring__ is a verb.<br> \n",
    "We use [Textblob](https://textblob.readthedocs.io/en/dev/) for __speech tagging__ which means to assign to each word in a sentence what kind of word that given word is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('my', 'PRP$'),\n",
       " ('first', 'JJ'),\n",
       " ('sentence', 'NN')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob( mytext )\n",
    "\n",
    "blob.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last output, we see how each word in __mytext__ gets its word class assigned. We can use this to simplify a sentence. For this, we identify __verbs__ by the leading V and __nouns__ by the leading N in their tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This be another sentence with more word contain some meaning but void of any real message or hidden message \n"
     ]
    }
   ],
   "source": [
    "mytext2 = \"This is another sentence, with more words, containing some meaning, \\\n",
    "but void of any real message or hidden messages\"\n",
    "\n",
    "blob2 = TextBlob( mytext2 )\n",
    "\n",
    "tags = blob2.tags\n",
    "\n",
    "mytext2_transformed = \"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for t in tags:\n",
    "    \n",
    "    word = t[ 0 ]\n",
    "    \n",
    "    selector = t[ 1 ][ 0 ]\n",
    "    \n",
    "    if selector == 'N':\n",
    "        \n",
    "        word = lemma.lemmatize( word, 'n' )\n",
    "        \n",
    "    if selector == 'V':\n",
    "        \n",
    "        word = lemma.lemmatize( word, 'v' )\n",
    "        \n",
    "    mytext2_transformed = mytext2_transformed + word + \" \"\n",
    "    \n",
    "    \n",
    "    \n",
    "print( mytext2_transformed )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice, that due to our lazy implementation, we lost interpunction, but we succssfully mapped the original sentence to a sentence which only consists of the word stems. This is important, because in practical problems, we can build a languge model using only the words from the dictionary, and not every possible word. This makes the model more sparse. But even though we employ a sparse model, as you can see from the last output, we do not lose too much meaning.<br>\n",
    "Now, we have covered basic natural language modelling and close the session.<br>\n",
    "Class dismissed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
