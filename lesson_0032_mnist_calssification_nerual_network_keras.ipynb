{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 0032 - MNIST Classification Neural Network Keras\n",
    "In this lesson, we will reproduce [lesson 0014](https://github.com/Mathhead/Lessons-in-Machine-Learning/blob/master/lesson_0014_mnist_classification_neural_network.ipynb) using [keras](https://keras.io/).<br>\n",
    "For this end, we will start by stealing code from [lesson 0014](https://github.com/Mathhead/Lessons-in-Machine-Learning/blob/master/lesson_0014_mnist_classification_neural_network.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.set_random_seed( 1234567890 )\n",
    "\n",
    "print( tf.__version__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "( train_x, train_y ),( test_x, test_y ) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed( 1234567890 )\n",
    "\n",
    "print( np.__version__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print( matplotlib.__version__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean( train_x )\n",
    "\n",
    "sigma = np.std( train_x )\n",
    "\n",
    "\n",
    "\n",
    "train_x = ( train_x - mu ) / sigma\n",
    "\n",
    "test_x = ( test_x - mu ) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we introduce, that __train_y_f__ and __test_y_f__ have integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_f = np.zeros( shape = [ 60000, 28 * 28 ] )\n",
    "\n",
    "train_y_f = np.zeros( shape = [ 60000, 10 ], dtype = np.int32 )\n",
    "\n",
    "test_x_f = np.zeros( shape = [ 10000, 28 * 28 ] )\n",
    "\n",
    "test_y_f = np.zeros( shape = [ 10000, 10 ], dtype = np.int32  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range( 60000 ):\n",
    "    \n",
    "    train_y_f[ i, np.int32( train_y[ i ] ) ] = 1\n",
    "    \n",
    "    \n",
    "for i in range( 10000 ):\n",
    "    \n",
    "    test_y_f[ i, np.int32( test_y[ i ] ) ] = 1\n",
    "    \n",
    "\n",
    "for i in range( 60000 ):\n",
    "    \n",
    "    dummy = np.array( train_x[ i ] )\n",
    "    \n",
    "    train_x_f[ i, : ] = dummy.flatten()\n",
    "    \n",
    "    \n",
    "for i in range( 10000 ):\n",
    "    \n",
    "    dummy = np.array( test_x[ i ] )\n",
    "    \n",
    "    test_x_f[ i ] = dummy.flatten()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "train_x = train_x_f\n",
    "\n",
    "train_y = train_y_f\n",
    "\n",
    "test_x = test_x_f\n",
    "\n",
    "test_y = test_y_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will continue \"stealing\" code from [lesson 0014](https://github.com/Mathhead/Lessons-in-Machine-Learning/blob/master/lesson_0014_mnist_classification_neural_network.ipynb), but there, we built the model in tensorflow, and here, we will build it in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "print( keras.__version__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining the variable __network__ as a sequential model using [keras' Sequential class](https://keras.io/getting-started/sequential-model-guide/).<br>\n",
    "We add a $28*28$ nodes layer by using [add](https://keras.io/getting-started/sequential-model-guide/), [Dense layers](https://keras.io/layers/core/) and [LeakyReLu](https://keras.io/layers/advanced-activations/) as activation function, where we use the __alpha__ value as proposed by keras.<br>\n",
    "We then add the softmax layer using the [softmax activation function](https://keras.io/activations/).<br>\n",
    "We then compile the __network__ using [compile](https://keras.io/models/model/). Since keras does not offer gradient descent which we used in [lesson 0014](https://github.com/Mathhead/Lessons-in-Machine-Learning/blob/master/lesson_0014_mnist_classification_neural_network.ipynb), we employ [stochastic gradient descent](https://keras.io/optimizers/) as the optimization algorithm, and set the other options like in [lesson 0014](https://github.com/Mathhead/Lessons-in-Machine-Learning/blob/master/lesson_0014_mnist_classification_neural_network.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rhopi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rhopi\\Anaconda3\\lib\\site-packages\\keras\\activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential()\n",
    "\n",
    "network.add( layers.Dense( 28 * 28, activation = layers.LeakyReLU( alpha = 0.3 ), \n",
    "                          input_shape = ( 28 * 28, ) ) )\n",
    "\n",
    "network.add( layers.Dense( 10, activation = \"softmax\" ) )\n",
    "\n",
    "network.compile( optimizer = keras.optimizers.SGD( lr = 0.1, momentum = 0.0, decay = 0.0, nesterov = False ),\n",
    "               loss = \"categorical_crossentropy\", metrics = [ \"accuracy\" ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [lesson 0014](https://github.com/Mathhead/Lessons-in-Machine-Learning/blob/master/lesson_0014_mnist_classification_neural_network.ipynb), we trained the model for $10000$ iterations with $100$ training data per iteration. In keras, one does not consider the number of iterations but the number of epochs. One epoch means, that each training data item has been considered once. This means, that $600$ iterations from [lesson 0014](https://github.com/Mathhead/Lessons-in-Machine-Learning/blob/master/lesson_0014_mnist_classification_neural_network.ipynb) are one epoch. Therefore, we train for $16$ epochs. For this, we employ the function [fit](https://keras.io/models/model/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rhopi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/16\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.2879 - acc: 0.9170\n",
      "Epoch 2/16\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1492 - acc: 0.9573\n",
      "Epoch 3/16\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1089 - acc: 0.9689: 0s - loss: 0.1122 - \n",
      "Epoch 4/16\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0850 - acc: 0.9757\n",
      "Epoch 5/16\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0694 - acc: 0.9805\n",
      "Epoch 6/16\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0573 - acc: 0.9834\n",
      "Epoch 7/16\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0483 - acc: 0.9867\n",
      "Epoch 8/16\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0408 - acc: 0.9888\n",
      "Epoch 9/16\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0345 - acc: 0.9906\n",
      "Epoch 10/16\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0293 - acc: 0.9926\n",
      "Epoch 11/16\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0251 - acc: 0.9937\n",
      "Epoch 12/16\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0216 - acc: 0.9949\n",
      "Epoch 13/16\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0184 - acc: 0.9961\n",
      "Epoch 14/16\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0162 - acc: 0.9969\n",
      "Epoch 15/16\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0138 - acc: 0.9978\n",
      "Epoch 16/16\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0120 - acc: 0.9983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28195f33e48>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit( train_x, train_y, epochs = 16, batch_size = 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, that we reached an accuracy of whooping $99.8\\%$ on the training set.<br>\n",
    "We employ the function [evaluate](https://keras.io/models/model/) to find out, how well we perform on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 32us/step\n",
      "We reached an accuracy of 98.16% on the test set.\n"
     ]
    }
   ],
   "source": [
    "_, acc = network.evaluate( test_x, test_y )\n",
    "\n",
    "print( \"We reached an accuracy of \" + str( 100 * acc ) + \"% on the test set.\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are done with building our first keras model.<br>\n",
    "Class dismissed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
